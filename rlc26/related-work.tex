\subsection{Related work}
\begin{enumerate}
    \item Fairly comprehensive reference survey of multi-objective RL:~\citet{hayes2022practicalguidemorl}
    \item Reference for definitions of multi agent RL:~\citet{bucsoniu2010multiagentoverview}.
    \item W-learning and more recent Deep W learning:~\citet{humphrys1995w,rosero2024multi}
    \item General multi-objective deep RL works.~\citet{nguyen2020multi} introduce a multi-policy DQN algorithm to learn multiple policies in parallel such that they have access to any possible linear weighting of the objectives. Also has a good lit review from where I got a few of these other citations.
    \item Multi objective Q learning (tabular):~\citet{van2013scalarized},~\citet{van2014multi} for pareto Q learning (maintains a set of policies and updates them), local search after normal learning:~\citet{van2014novel}
    \item Use gradient approximation to iteratively build parametrized policies representing pareto frontier~\citet{pirotta2015multi}
    \item Maximin (fairness) MORL:~\citet{park2024max,byeon2025multi}, fair deep RL:~\citet{siddique2020learning}.
    \item These two are only vaguely related and look at decomposing an objective into multiple smaller objectives: Q-function decomposition for a single agent with multiple reward components:~\citet{russell2003q} and on a similar note hybrid reward architecture:~\citet{van2017hybrid}
\end{enumerate}
