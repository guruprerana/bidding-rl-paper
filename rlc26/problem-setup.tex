\section{A Multi-Agent Bidding Approach for Multi-Objective RL}
\begin{definition}[\momdp]
    A multi-objective Markov decision process (\momdp) with \(m \in \Z_{>0}\) objectives is specified by a tuple \(\mathcal{M} = (S, A, T, R, \mu_0)\), where
    \begin{itemize}
        \item \(S\) is the set of states,
        \item \(A\) is the set of actions,
        \item \(T : S \times A \to \dist(S)\) is the transition function mapping a state-action pair to a distribution over states,
        \item \(R : S \times A \times S \to \R^m\) is the reward function with each output component corresponding to the different objectives, and
        \item \(\mu_0 \in \dist(S)\) is the initial state distribution.
    \end{itemize}
\end{definition}

A \textit{policy} in an \momdp~\(\mathcal{M}\) is a function \(\pi : (S \times A)^* \times S \to \dist(A)\) that maps a history of state-action pairs and the current state to a distribution over actions. 

\begin{definition}[\mabmdp]
    Let \(\mathcal{M} = (S, A, T, R, \mu_0)\) be an \momdp~with \(m\) objectives and let \(b \in \Z_{>0}\) be the bid upper bound. Also, define \(M = \{1, \dots, m\}\) be indices of the \(m\) agents corresponding to the \(m\) objectives along with \(\bot\) representing a null agent. Lastly, let \(B = \{0, \dots, b\}\) be the range of bids and \(\rho > 0\) be the bid penalty factor.
    We define the multi-agent bidding Markov decision process (\mabmdp) as a tuple \(\mathcal{B}_{\mathcal{M}} = (\hat S, \hat{A}, \hat{T}, P, \hat{R}, \hat \mu_0)\) where
    \begin{itemize}
        \item \(\hat S = M \times S\) is the new state space augmented with the index of the agent that won the previous round of bidding,
        \item \(\hat{A} = A^m \times B^m\) represents the action space of the \(m\) agents in which each agent selects an action from \(A\) and a bid from \(B\),
        \item \(\hat{T} : \hat S \times \hat{A} \to \dist(\hat S)\) is the new transition function defined as,
        \begin{equation*}
            \hat{T}((\_, s), (\vec a, \vec b)) \coloneqq \frac{1}{\abs{B_{\max}}}\sum_{i \in B_{\max}} (T(s, a_i), i)
        \end{equation*}
        where \(B_{\max} \coloneqq \{i \mid b_i = \max \{b_1, \dots, b_m\}\}\) is the set of agents with maximal bids. The tuple \((T(s, a_i), i)\) represents the distribution over \(\hat S\) induced by the original transition function \(T\) such that the second component is fixed, and the weighted sum represents taking the weighted sums of the distributions over \(\hat S\).
        \item \(P : \hat A \times M \to \R^m\) is the bidding penalty for the \(m\) agents and the second component is the index of the agent that won the bidding.
        \item \(\hat{R} : \hat S \times \hat{A} \times \hat S \to \R^m\) is the reward function for the \(m\) agents with
        \begin{equation*}
            \hat{R}_k((\_, s_0), (\vec a, \vec b), (i, s)) \coloneqq R_k(s_0, a_i, s) - P_k((\vec a, \vec b), i)
        \end{equation*}
        where \(i \in M\) is the index of the agent that won the bid and chose the action.
        \item \(\hat \mu_0 \coloneqq (\mu_0, 1)\) is the initial state distribution over \(\hat S\) induced by \(\mu_0\) and the second component is fixed to be \(1\) without loss of generality.
    \end{itemize}
\end{definition}

Given an \mabmdp~\(\mathcal{B}_{\mathcal{M}}\), a \textit{policy} for each agent indexed by \(i \in \{1, \dots, m\}\) takes a similar form: \(\pi_i : (\hat S \times \hat A)^* \times \hat S \to \hat A\). Intuitively, a state \((i, s) \in \hat S\) encodes the agent that won the bidding and chose the action to reach \(s\) in the previous step. At each step, each of the agents choose an action and a bid, and an action amongst the set of highest bidders is chosen uniformly at random. The reward function includes a penalty term that captures the desired bidding mechanism.
