\section{Compositional RL on Multi-Objective MDPs}

Mainstream RL algorithms consider Markov decision processes (MDP) equipped with a \textit{single} reward function, pertaining to a single task or \textit{objective} for the system.
In reality, a majority of real-world applications of RL requires satisfying multiple, partly contradictory objectives.
We model such multi-objective decision-making problems using multi-objective MDPs (MO-MDP), as formally defined below.
Intuitively, an MO-MDP has the exact same syntax as a regular MDP, except that it now has multiple reward functions pertaining to the different objectives.

\begin{definition}[\momdp]
    A multi-objective Markov decision process (\momdp) with \(m \in \Z_{>0}\) objectives is specified by a tuple \(\mathcal{M} = (S, A, T, R, \mu_0)\), where
    \begin{itemize}
        \item \(S\) is the set of states,
        \item \(A\) is the set of actions,
        \item \(T : S \times A \to \dist(S)\) is the transition function mapping a state-action pair to a distribution over the successor states,
        \item \(\{R_i : S \times A \times S \to \R\}_{i\in [1;m]}\) is the set of reward functions pertaining to different objectives, and
        \item \(\mu_0 \in \dist(S)\) is the initial state distribution.
    \end{itemize}
\end{definition}

The notions of policies and paths induced by them are exactly the same as in classical MDPs, which we briefly recall below.
A \textit{policy} in an \momdp~\(\mathcal{M}\) is a function \(\pi : (S \times A)^* \times S \to \dist(A)\) that maps a history of state-action pairs and the current state to a distribution over actions. 
A \emph{path} on $\M$ induced by $\pi$ is a sequence $(s_0,a_0)(s_1,a_1),\ldots\in (S\times A)^\infty$ such that for every $i\geq 0$, $\pi((s_0,a_0)\ldots (s_i,a_i),s_{i+1})(a_{i+1}) > 0$ and $T(s_i,a_i)(s_{i+1})>0$.
A path can be either finite or infinite, and we will write $\paths(\M,\pi)$ to denote the set of all infinite paths fo $\M$ induced by $\pi$.
Given a finite path $\rho=(s_0,a_0)\ldots (s_i,a_i)$, the probability that $\rho$ occurs is given by: $\mu_0(s_0)\cdot\prod_{j=0}^{i-1} T(s_j,a_j)(s_{j+1})\cdot \pi((s_0,a_0)\ldots (s_j,a_j),s_{j+1})(a_{j+1})$.
This can be extended to a probability measure over the set of all infinite paths in $\M$ using standard constructions, which can be found in the literature~\citep{baier2008principles}.
Given a measurable set of paths $\Omega$ and a function $f\colon \paths(\M,\pi)\to \mathbb{R}$, we will write $\P^{\M,\pi}[\Omega]$ and $\E^{\M,\pi}[f]$ to denote, respectively, the probability measure of $\Omega$ and the expected value of $f$ evaluated over random infinite paths.

We will use the standard discounted reward objectives.
Suppose $\gamma \in [0,1]$ is a fixed discounting factor.
Let $\theta = r_0r_1\ldots\in \R^\omega$ be an infinite sequence of real numbers.
Define the discounted sum function: $f_{\mathrm{ds}}(\theta)\coloneqq\sum_{i=0}^\infty \gamma^i\cdot r_i$.
Then, the \emph{value} of the policy $\rho$ for $\M$ is the expected value of the discounted sum of rewards we can secure by executing $\rho$ on $\M$, i.e, $\val^{\M}(\pi) = \E^{\M,\pi}[f_{\mathrm{ds}}]$.




